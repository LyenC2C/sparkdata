#coding:utf-8
__author__ = 'wrt'
import sys
from pyspark import SparkContext
sc = SparkContext(appName="repair s_price_zero")

def f(line):
    ss = line.strip().split("\001")
    if ss[3] == '0.0' or ss[3] == '0':
        ss[3] = ss[2]
    result = "\001".join(ss)
    return result
ds_list = [
"ds=20151026",
"ds=20151027",
"ds=20151028"
"ds=20151029",
"ds=20151030",
"ds=20151102",
"ds=20151104",
"ds=20151105",
"ds=20151106",
"ds=20151107",
"ds=20151108",
"ds=20151109",
"ds=20151110",
"ds=20151111",
"ds=20151112",
"ds=20151113",
"ds=20151114",
"ds=20151115",
"ds=20151116",
"ds=20151117",
"ds=20151118",
"ds=20151119",
"ds=20151120",
"ds=20151121",
"ds=20151122",
"ds=20151123",
"ds=20151124",
"ds=20151125",
"ds=20151126",
"ds=20151127",
"ds=20151128",
"ds=20151129",
"ds=20151130",
"ds=20151201",
"ds=20151202",
"ds=20151203",
"ds=20151204",
"ds=20151205",
"ds=20151206",
"ds=20151207",
"ds=20151208",
"ds=20151209",
"ds=20151210",
"ds=20151211",
"ds=20151212",
"ds=20151213",
"ds=20151214",
"ds=20151215",
"ds=20151216",
"ds=20151217",
"ds=20151218",
"ds=20151219",
"ds=20151220",
"ds=20151221",
"ds=20151222",
"ds=20151223",
"ds=20151224",
"ds=20151225",
"ds=20151226",
"ds=20151227",
"ds=20151228",
"ds=20151229",
"ds=20151230",
"ds=20151231",
"ds=20160101",
"ds=20160102",
"ds=20160103",
"ds=20160104",
"ds=20160105",
"ds=20160106",
"ds=20160107",
"ds=20160108",
"ds=20160112",
"ds=20160113",
"ds=20160114",
"ds=20160115",
"ds=20160116",
"ds=20160117",
"ds=20160118",
"ds=20160119",
"ds=20160120",
"ds=20160121",
"ds=20160122",
"ds=20160123",
"ds=20160124",
"ds=20160125",
"ds=20160126",
"ds=20160127",
"ds=20160128",
"ds=20160129",
"ds=20160130",
"ds=20160131",
"ds=20160201",
"ds=20160202",
"ds=20160203",
"ds=20160204",
"ds=20160205",
"ds=20160206",
"ds=20160207",
"ds=20160208",
"ds=20160209",
"ds=20160210",
"ds=20160211",
"ds=20160212",
"ds=20160213",
"ds=20160214",
"ds=20160215",
"ds=20160216",
"ds=20160217",
"ds=20160218",
"ds=20160219",
"ds=20160220",
"ds=20160221",
"ds=20160222",
"ds=20160223",
"ds=20160224",
"ds=20160225",
"ds=20160226",
"ds=20160227",
"ds=20160228",
"ds=20160229",
"ds=20160301",
"ds=20160302",
"ds=20160303",
"ds=20160304",
"ds=20160305",
"ds=20160306",
"ds=20160307",
"ds=20160308",
"ds=20160309",
"ds=20160310",
"ds=20160311",
"ds=20160312",
"ds=20160313",
"ds=20160314",
"ds=20160315",
"ds=20160316"
]
for ds in ds_list:
    s1 = "/hive/warehouse/wlbase_dev.db/t_base_ec_item_sale_dev/" + ds
    rdd = sc.textFile(s1).map(lambda x:f(x))
    s2 = "/user/wrt/t_base_ec_item_sale_dev_tmp/" + ds
    rdd.saveAsTextFile(s2)

sc.stop()

#spark-submit  --executor-memory 20G  --driver-memory 20G  --total-executor-cores 200 repair_s_price_zero.py